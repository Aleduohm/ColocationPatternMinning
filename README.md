# ColocationPatternMinning
[![Blog](https://img.shields.io/badge/Github-@awei_lwj-blue.svg?style=social&logo=Github)](https://github.com/awei-lwj)&nbsp;&nbsp;![Paper Notebook](https://img.shields.io/badge/paper-Notebook-blue.svg?style=social&&logo=paperspace)&nbsp;&nbsp;![Stars Thanks](https://img.shields.io/badge/Stars-Thank-brightgreen.svg?style=social&logo=trustpilot)&nbsp;&nbsp;![Submission Welcome](https://img.shields.io/badge/Submission-Welcome-brightgreen.svg?style=social&logo=gitlab)&nbsp;&nbsp;![Issues Welcome](https://img.shields.io/badge/Issues-Welcome-brightgreen.svg?style=social&logo=feathub)&nbsp;&nbsp;![WeChat](https://img.shields.io/badge/WeChat-阿伟-brightgreen.svg?style=social&logo=WeChat) 
</div>

记录awei在学习NLP过程中的感觉比较OK的论文，并通过basic method上的three-pass-efficiency-reading-paper方法对这些论文进行分析or复现。

欢迎各位commit自己觉得比较nice的论文，推荐大家按照我的template进行阅读并且按照Tips进行提交

> 由于平时比较忙碌，如果awei没有添加Abstract的说明看的是别人的笔记,自己并没有仔细复盘

---

![Python](https://img.shields.io/badge/Python-Environment-brightdark.svg?style=social&logo=python)

Environment Setup

```python
pip install -r requirements.txt
```

---

![Tips](https://img.shields.io/badge/Submissions-Tips-brightdark.svg?style=social&logo=reverbnation)

Tips：

- 请严格按照Submission Formation的格式，对提交的Notebook进行描述

# Submission Formation

## Paper

![Git](https://img.shields.io/badge/Git-Formulation-brightdark.svg?style=social&logo=git)

```
< number | label | [paper](link) | Cite:(MLA/ACL) >

(Optional)Abstract：
(Optional)Keywords:
(Optional)Tips:
(Optional)Related Works:
(Optional)Recreate or Questions:
(Optional)Code in Github:[link](link)
```

Add the paper citation in Reference

```
Cite(MLA): 
```

![Tips](https://img.shields.io/badge/New_label-Tips-brightdark.svg?style=social&logo=reverbnation)

**If there is the new label,you should submit the keyword**

## Code tutorial

参考这个格式进行处理
[https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp](https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp)

# Papers List

![Keywords](https://img.shields.io/badge/Papers-Keywords-brightdark.svg?style=social&logo=contentful)


<div align="center">


&nbsp;&nbsp;[Methods](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[Image description generation](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[Information bottlenecks](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[NLP Classical Model](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)<br>&nbsp;&nbsp;[Attention](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[Text-image-generation](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[Capsule Network](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[BERT](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[Knowledge distillation](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[Mixture of Experts](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)<br>[Parameter-Efficient Fine-Tuning (PEFT)](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[Hugging Face Tutorial](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)&nbsp;&nbsp;•&nbsp;&nbsp;[Semantic Textual Similarity](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)&nbsp;&nbsp;•&nbsp;&nbsp;[Contrast learning](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)<br>[Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)&nbsp;&nbsp;•&nbsp;&nbsp;[Data Argument](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[MultiModal Knowledge Graph Compeletion](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[Graph Neural Networks](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)<br>[MultiModal Entity Link](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[MultiModal](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)&nbsp;&nbsp;•&nbsp;&nbsp;[Large Language Models](https://github.com/awei-lwj/Dive-Into-Papers-Reading-Reproduction)

</div>
